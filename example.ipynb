{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"02bfa0a8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Comparing Explanation Methods with deit_tiny_finetuned\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook loads a finetuned deit_tiny_patch16_224 model (with a custom head and weights), then runs several explanation methods on each image from a dataset (e.g., COVID-Q). It creates a composite image comparing the different explanation masks.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Make sure that all your functions (such as `deit_tiny_finetuned`, `run_explanation`, `show_mask_on_image`, etc.) are available (e.g., in your PYTHONPATH) before running this notebook.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"7ac3ad3b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import glob\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"from torchvision import transforms\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import your explanation functions and model loading functions\\n\",\n",
    "    \"# Adjust these imports to point to your modules\\n\",\n",
    "    \"from vit_rollout import VITAttentionRollout\\n\",\n",
    "    \"from vit_grad_rollout import VITAttentionGradRollout\\n\",\n",
    "    \"from vit_explainability import VITTransformerExplainability\\n\",\n",
    "    \"from vit_LRPmimic import VITTransformerLRPmimic\\n\",\n",
    "    \"from vit_LRPexact import load_model_LRP, VITTransformerLRPexact\\n\",\n",
    "    \"from load_deit import load_deit  # if used in your fine-tuning function\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Custom finetuned model loader\\n\",\n",
    "    \"def deit_tiny_finetuned(head_weights_path, pretrained=True, **kwargs):\\n\",\n",
    "    \"    from vit_explainability import VisionTransformer, _cfg  # adjust as needed\\n\",\n",
    "    \"    import torch.nn as nn\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Define a custom linear layer with relprop\\n\",\n",
    "    \"    class LinearRelprop(nn.Linear):\\n\",\n",
    "    \"        def forward(self, x):\\n\",\n",
    "    \"            self.input = x\\n\",\n",
    "    \"            return super().forward(x)\\n\",\n",
    "    \"        def relprop(self, cam, epsilon=1e-6, **kwargs):\\n\",\n",
    "    \"            z = torch.matmul(self.input, self.weight.t()) + epsilon\\n\",\n",
    "    \"            s = cam / z\\n\",\n",
    "    \"            c = torch.matmul(s, self.weight)\\n\",\n",
    "    \"            return self.input * c\\n\",\n",
    "    \"\\n\",\n",
    "    \"    model = VisionTransformer(\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        embed_dim=192,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=3,\\n\",\n",
    "    \"        mlp_ratio=4,\\n\",\n",
    "    \"        qkv_bias=True,\\n\",\n",
    "    \"        **kwargs\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    model.default_cfg = _cfg()\\n\",\n",
    "    \"    if pretrained:\\n\",\n",
    "    \"        checkpoint = torch.hub.load_state_dict_from_url(\\n\",\n",
    "    \"            url=\\\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\\\",\\n\",\n",
    "    \"            map_location=\\\"cpu\\\",\\n\",\n",
    "    \"            check_hash=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        model.load_state_dict(checkpoint[\\\"model\\\"])\\n\",\n",
    "    \"    in_features = model.head.in_features\\n\",\n",
    "    \"    model.head = LinearRelprop(in_features, 2)\\n\",\n",
    "    \"    head_state_dict = torch.load(head_weights_path, map_location=torch.device('cpu'))\\n\",\n",
    "    \"    model.head.load_state_dict(head_state_dict)\\n\",\n",
    "    \"    return model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Function to load model based on the model name\\n\",\n",
    "    \"def load_model(model_name, parameters):\\n\",\n",
    "    \"    print(\\\"Loading model:\\\", model_name)\\n\",\n",
    "    \"    if model_name == \\\"deit_tiny_finetuned\\\":\\n\",\n",
    "    \"        model = deit_tiny_finetuned(parameters[\\\"weights_path\\\"], pretrained=parameters[\\\"pretrained\\\"])\\n\",\n",
    "    \"    elif model_name == \\\"deit_tiny\\\":\\n\",\n",
    "    \"        model = torch.hub.load('facebookresearch/deit:main', \\\"deit_tiny_patch16_224\\\", parameters[\\\"pretrained\\\"])\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        raise ValueError(\\\"Unknown model: {}\\\".format(model_name))\\n\",\n",
    "    \"    return model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# A simple function to overlay a heatmap on an image\\n\",\n",
    "    \"def show_mask_on_image(img, mask):\\n\",\n",
    "    \"    img = np.float32(img) / 255\\n\",\n",
    "    \"    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\\n\",\n",
    "    \"    heatmap = np.float32(heatmap) / 255\\n\",\n",
    "    \"    cam = heatmap + np.float32(img)\\n\",\n",
    "    \"    cam = cam / np.max(cam)\\n\",\n",
    "    \"    return np.uint8(255 * cam)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# A helper function to create a composite image from the original and masks\\n\",\n",
    "    \"def create_composite(original_img, results_dict):\\n\",\n",
    "    \"    orig = np.array(original_img)  # in RGB\\n\",\n",
    "    \"    orig = cv2.cvtColor(orig, cv2.COLOR_RGB2BGR)\\n\",\n",
    "    \"    orig = cv2.resize(orig, (224, 224))\\n\",\n",
    "    \"    composite_images = [orig]\\n\",\n",
    "    \"    for method, mask in results_dict.items():\\n\",\n",
    "    \"        m = cv2.resize(mask, (orig.shape[1], orig.shape[0]))\\n\",\n",
    "    \"        overlay = show_mask_on_image(orig, m)\\n\",\n",
    "    \"        cv2.putText(overlay, method, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\\n\",\n",
    "    \"        composite_images.append(overlay)\\n\",\n",
    "    \"    comp = np.hstack(composite_images)\\n\",\n",
    "    \"    return comp\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define a dictionary of methods to compare\\n\",\n",
    "    \"methods = [\\\"attention\\\", \\\"gradient\\\", \\\"explainability\\\", \\\"LRP_mimic\\\", \\\"LRP_exact\\\"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Parameters for the model, as a dictionary (this was originally passed as a JSON string)\\n\",\n",
    "    \"model_parameters = {\\\"pretrained\\\": True, \\\"weights_path\\\": \\\"weights/deit_tiny_head_weights.pth\\\"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set device\\n\",\n",
    "    \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the finetuned model\\n\",\n",
    "    \"model = load_model(\\\"deit_tiny_finetuned\\\", model_parameters)\\n\",\n",
    "    \"model = model.to(device)\\n\",\n",
    "    \"model.eval()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Transformation for the input images\\n\",\n",
    "    \"transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.Resize((224, 224)),\\n\",\n",
    "    \"    transforms.ToTensor(),\\n\",\n",
    "    \"    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a dummy args dictionary (simulate command-line arguments)\\n\",\n",
    "    \"args = {\\n\",\n",
    "    \"    \\\"category_index\\\": None,\\n\",\n",
    "    \"    \\\"discard_ratio\\\": 0.9,\\n\",\n",
    "    \"    \\\"head_fusion\\\": \\\"max\\\",\\n\",\n",
    "    \"    \\\"attention_layer_name\\\": \\\"attn_drop\\\",\\n\",\n",
    "    \"    \\\"model_name\\\": \\\"deit_tiny_finetuned\\\"\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# %% [markdown]\\n\",\n",
    "    \"# ## Process Images and Compare Explanation Methods\\n\",\n",
    "    \"# Loop over a subset of images from the dataset and generate composite images.\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get list of images from dataset directory (adjust extension if necessary)\\n\",\n",
    "    \"dataset_dir = \\\"/path/to/covidqu\\\"  # update to your dataset directory\\n\",\n",
    "    \"output_dir = \\\"comparisons\\\"\\n\",\n",
    "    \"os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "    \"image_files = glob.glob(os.path.join(dataset_dir, \\\"*.[jp][pn]g\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Found {len(image_files)} images.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Process only a few images for demonstration (e.g., first 5 images)\\n\",\n",
    "    \"for img_path in image_files[:5]:\\n\",\n",
    "    \"    print(\\\"Processing\\\", img_path)\\n\",\n",
    "    \"    img = Image.open(img_path).convert(\\\"RGB\\\")\\n\",\n",
    "    \"    input_tensor = transform(img).unsqueeze(0).to(device)\\n\",\n",
    "    \"    results = {}\\n\",\n",
    "    \"    for method in methods:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            # Here, we assume run_explanation is defined and imported from your module\\n\",\n",
    "    \"            from vit_explain_modulable import run_explanation\\n\",\n",
    "    \"            mask, _ = run_explanation(method, model, input_tensor, args)\\n\",\n",
    "    \"            results[method] = mask\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Method {method} failed on {img_path}: {e}\\\")\\n\",\n",
    "    \"    composite = create_composite(img, results)\\n\",\n",
    "    \"    base_name = os.path.basename(img_path)\\n\",\n",
    "    \"    out_path = os.path.join(output_dir, f\\\"composite_{base_name}\\\")\\n\",\n",
    "    \"    cv2.imwrite(out_path, composite)\\n\",\n",
    "    \"    print(\\\"Saved composite image to\\\", out_path)\\n\",\n",
    "    \"    plt.figure(figsize=(12,6))\\n\",\n",
    "    \"    plt.imshow(cv2.cvtColor(composite, cv2.COLOR_BGR2RGB))\\n\",\n",
    "    \"    plt.axis(\\\"off\\\")\\n\",\n",
    "    \"    plt.title(f\\\"Comparison for {base_name}\\\")\\n\",\n",
    "    \"    plt.show()\\n\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.x\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
